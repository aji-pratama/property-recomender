{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder\\\n",
    "    .master('local')\\\n",
    "    .config('spark.mongodb.input.uri', 'mongodb://127.0.0.1:27017/propertify')\\\n",
    "    .config('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/propertify')\\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.2.1')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = SparkContext.getOrCreate(\"local\")\n",
    "locale = spark._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "\n",
    "property_df = spark.read\\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"database\", \"learns\")\\\n",
    "    .option(\"collection\", \"crawler\")\\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                 _id|               title|                 url|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[5eb51e052fcfa6e7...|How to find the c...|/questions/616754...|\n",
      "|[5eb51e052fcfa6e7...|MySQL to return l...|/questions/616754...|\n",
      "|[5eb51e052fcfa6e7...|mysql - Count of ...|/questions/616754...|\n",
      "|[5eb51e052fcfa6e7...|Upload multiple f...|/questions/616754...|\n",
      "|[5eb51e052fcfa6e7...|How to resize a f...|/questions/616754...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "property_df.printSchema()\n",
    "property_df.show(5)\n",
    "property_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler\n",
    "from pyspark.ml.feature import Word2Vec, Word2VecModel\n",
    "from pyspark.ml.feature import IDF, HashingTF\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'title', outputCol = 'token')\n",
    "stopWordsRemover = StopWordsRemover(inputCol = 'token', outputCol = 'nostopwrd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashingTF = HashingTF(inputCol=\"nostopwrd\", outputCol=\"rawFeature\" qw  cwkcw  \n",
    "countVectorizer = CountVectorizer(inputCol=\"nostopwrd\", outputCol=\"rawFeature\")\n",
    "iDF = IDF(inputCol=\"rawFeature\", outputCol=\"idf_vec\")\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopWordsRemover, countVectorizer, iDF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_mdl = pipeline.fit(property_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- nostopwrd: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeature: vector (nullable = true)\n",
      " |-- idf_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "property_trf_df = pipeline_mdl.transform(property_df)\n",
    "property_trf_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 _id|               title|                 url|             idf_vec|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|[5eb51e052fcfa6e7...|How to find the c...|/questions/616754...|(281,[5,14,15,56,...|\n",
      "|[5eb51e052fcfa6e7...|MySQL to return l...|/questions/616754...|(281,[17,18,126,1...|\n",
      "|[5eb51e052fcfa6e7...|mysql - Count of ...|/questions/616754...|(281,[0,18,36,146...|\n",
      "|[5eb51e052fcfa6e7...|Upload multiple f...|/questions/616754...|(281,[12,23,46,58...|\n",
      "|[5eb51e052fcfa6e7...|How to resize a f...|/questions/616754...|(281,[15,39,131],...|\n",
      "|[5eb51e052fcfa6e7...|How to convert fo...|/questions/616754...|(281,[9,19,50,61,...|\n",
      "|[5eb51e052fcfa6e7...|Android BaseAdapt...|/questions/616754...|(281,[22,44,54,89...|\n",
      "|[5eb51e052fcfa6e7...|Xpath results in ...|/questions/616754...|(281,[30,118,203,...|\n",
      "|[5eb51e052fcfa6e7...|How to migrate da...|/questions/616754...|(281,[0,43,96,142...|\n",
      "|[5eb51e052fcfa6e7...|How can I use Lar...|/questions/616754...|(281,[3,17,67,71,...|\n",
      "|[5eb51e052fcfa6e7...|Error while conve...|/questions/616754...|(281,[1,2,4,10,37...|\n",
      "|[5eb51e052fcfa6e7...|Obtain the label ...|/questions/616754...|(281,[24,26,60,17...|\n",
      "|[5eb51e052fcfa6e7...|Are amp pages wit...|/questions/616754...|(281,[49,65,80,10...|\n",
      "|[5eb51e052fcfa6e7...|How to specify an...|/questions/616754...|(281,[120,133,157...|\n",
      "|[5eb51e052fcfa6e7...|Struggling with r...|/questions/616754...|(281,[4,6,34,41,1...|\n",
      "|[5eb51e052fcfa6e7...|Selecting a progr...|/questions/616754...|(281,[32,83,102,2...|\n",
      "|[5eb51e052fcfa6e7...|How to get the va...|/questions/616754...|(281,[5,16,153,21...|\n",
      "|[5eb51e052fcfa6e7...|Why can't I print...|/questions/616754...|(281,[1,9,116,135...|\n",
      "|[5eb51e052fcfa6e7...|How to get specif...|/questions/616754...|(281,[5,11,16,57,...|\n",
      "|[5eb51e052fcfa6e7...|Why does the gene...|/questions/616754...|(281,[66,86,117,2...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "property_trf_df.select('_id', 'title', 'url', 'idf_vec').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / np.sqrt(np.dot(vec1, vec1)) / np.sqrt(np.dot(vec2, vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan\n",
    "\n",
    "def getBusinessDetails(in_bus):\n",
    "    \n",
    "    a = in_bus.alias(\"a\")\n",
    "    b = property_df.alias(\"b\")\n",
    "    \n",
    "    return a.join(b, col(\"a.property_id\") == col(\"b._id\"), 'inner').select([col('a.'+xx) for xx in a.columns] + [col('b.title'), col('b.url')]).orderBy(\"a.score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_property_vecs = property_trf_df.select('_id', 'idf_vec').rdd.map(lambda x: (x[0], x[1])).collect() #change Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "def get_keywords_recomendations(key_words, sim_bus_limit):\n",
    "    input_words_df = sc.parallelize([(0, key_words)]).toDF(['_id', 'title'])\n",
    "    input_words_df = pipeline_mdl.transform(input_words_df)\n",
    "    input_key_words_vec = input_words_df.select('idf_vec').collect()[0][0]    #change Word2Vec\n",
    "    sim_property_byword_rdd = sc.parallelize((i[0], float(cosine_sim(input_key_words_vec, i[1]))) for i in all_property_vecs)\n",
    "    sim_property_byword_df = spark.createDataFrame(sim_property_byword_rdd) \\\n",
    "         .withColumnRenamed('_1', 'property_id') \\\n",
    "         .withColumnRenamed('_2', 'score') \\\n",
    "         .orderBy(\"score\", ascending=False)\n",
    "    a = sim_property_byword_df.filter((f.col('score')>0) & (~f.isnan('score'))).limit(sim_bus_limit)\n",
    "    return getBusinessDetails(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+--------------------+\n",
      "|         property_id|              score|               title|                 url|\n",
      "+--------------------+-------------------+--------------------+--------------------+\n",
      "|[5eb51e052fcfa6e7...| 0.3719859549546635|MySQL to return l...|/questions/616754...|\n",
      "|[5eb51e052fcfa6e7...|0.30751922102747997|mysql - Count of ...|/questions/616754...|\n",
      "+--------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key_words = 'Mysql'\n",
    "\n",
    "keywords_recom_df = get_keywords_recomendations(key_words, 20)\n",
    "keywords_recom_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'property_id': {'oid': '5eb51e052fcfa6e75e3c002f'},\n",
       "  'score': 0.3719859549546635,\n",
       "  'title': 'MySQL to return list of people who are not already friends',\n",
       "  'url': '/questions/61675447/mysql-to-return-list-of-people-who-are-not-already-friends'},\n",
       " {'property_id': {'oid': '5eb51e052fcfa6e75e3c0030'},\n",
       "  'score': 0.30751922102747997,\n",
       "  'title': 'mysql - Count of days in a month with groupby Month data',\n",
       "  'url': '/questions/61675444/mysql-count-of-days-in-a-month-with-groupby-month-data'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "keywords_recom_df.toJSON().map(lambda j: json.loads(j)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
